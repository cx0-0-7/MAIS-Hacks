{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e0ecb2-9ef3-46bd-ad56-1d5561d93fc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ============================================================\n",
    "# # ðŸš— Uber Ride Category Prediction (For Your Dataset)\n",
    "# # ============================================================\n",
    "\n",
    "# # !pip install tensorflow pandas numpy scikit-learn matplotlib\n",
    "\n",
    "# # ============================================================\n",
    "# # 1ï¸âƒ£ Imports\n",
    "# # ============================================================\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "# from sklearn.utils import resample\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# # ============================================================\n",
    "# # 2ï¸âƒ£ Load Data\n",
    "# # ============================================================\n",
    "# df = pd.read_csv(\"/Volumes/workspace/mais_hacks/dataset/UberDataset.csv\")\n",
    "# print(\"âœ… Data loaded:\", df.shape)\n",
    "# print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# # ============================================================\n",
    "# # 3ï¸âƒ£ Basic Cleaning & Feature Engineering\n",
    "# # ============================================================\n",
    "# df[\"START_DATE\"] = pd.to_datetime(df[\"START_DATE\"], errors=\"coerce\")\n",
    "# df[\"END_DATE\"] = pd.to_datetime(df[\"END_DATE\"], errors=\"coerce\")\n",
    "\n",
    "# # Extract time-based features\n",
    "# df[\"hour\"] = df[\"START_DATE\"].dt.hour\n",
    "# df[\"dayofweek\"] = df[\"START_DATE\"].dt.dayofweek\n",
    "# df[\"month\"] = df[\"START_DATE\"].dt.month\n",
    "\n",
    "# # Drop missing target rows\n",
    "# df.dropna(subset=[\"CATEGORY\"], inplace=True)\n",
    "\n",
    "# # ============================================================\n",
    "# # 4ï¸âƒ£ Encode Categorical Variables\n",
    "# # ============================================================\n",
    "# label_encoders = {}\n",
    "# for col in [\"START\", \"STOP\", \"PURPOSE\"]:\n",
    "#     if col in df.columns:\n",
    "#         le = LabelEncoder()\n",
    "#         df[col] = le.fit_transform(df[col].astype(str))\n",
    "#         label_encoders[col] = le\n",
    "\n",
    "# # Encode target\n",
    "# target_le = LabelEncoder()\n",
    "# df[\"CATEGORY\"] = target_le.fit_transform(df[\"CATEGORY\"])\n",
    "# y = to_categorical(df[\"CATEGORY\"])\n",
    "\n",
    "# # ============================================================\n",
    "# # 5ï¸âƒ£ Select Features (no PRICE column in your dataset)\n",
    "# # ============================================================\n",
    "# features = [\"MILES\", \"START\", \"STOP\", \"PURPOSE\", \"hour\", \"dayofweek\", \"month\"]\n",
    "# X = df[features].fillna(0)\n",
    "\n",
    "# # Normalize\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # ============================================================\n",
    "# # 6ï¸âƒ£ Balance Dataset\n",
    "# # ============================================================\n",
    "# df_bal = pd.concat([pd.DataFrame(X_scaled, columns=features),\n",
    "#                     pd.Series(np.argmax(y, axis=1), name=\"CATEGORY\")], axis=1)\n",
    "\n",
    "# majority_class = df_bal[\"CATEGORY\"].mode()[0]\n",
    "# majority = df_bal[df_bal.CATEGORY == majority_class]\n",
    "# minority = df_bal[df_bal.CATEGORY != majority_class]\n",
    "\n",
    "# minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "# df_bal = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "# X_bal = df_bal[features]\n",
    "# y_bal = to_categorical(df_bal[\"CATEGORY\"])\n",
    "\n",
    "# # ============================================================\n",
    "# # 7ï¸âƒ£ Train/Test Split\n",
    "# # ============================================================\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_bal, y_bal, test_size=0.2, random_state=42)\n",
    "# print(f\"Training samples: {len(X_train)} | Test samples: {len(X_test)}\")\n",
    "\n",
    "# # ============================================================\n",
    "# # 8ï¸âƒ£ Neural Network\n",
    "# # ============================================================\n",
    "# model = keras.Sequential([\n",
    "#     layers.Input(shape=(X_train.shape[1],)),\n",
    "#     layers.Dense(64, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(32, activation='relu'),\n",
    "#     layers.BatchNormalization(),\n",
    "#     layers.Dropout(0.3),\n",
    "#     layers.Dense(y_train.shape[1], activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "# # ============================================================\n",
    "# # 9ï¸âƒ£ Training\n",
    "# # ============================================================\n",
    "# callbacks = [\n",
    "#     EarlyStopping(patience=10, restore_best_weights=True),\n",
    "#     ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)\n",
    "# ]\n",
    "\n",
    "# history = model.fit(\n",
    "#     X_train, y_train,\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     epochs=100,\n",
    "#     batch_size=32,\n",
    "#     callbacks=callbacks,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # ============================================================\n",
    "# # ðŸ”Ÿ Evaluation\n",
    "# # ============================================================\n",
    "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "# print(f\"\\nâœ… Final Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "# y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred_classes))\n",
    "\n",
    "# cm = confusion_matrix(y_true, y_pred_classes)\n",
    "# ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "#                        display_labels=target_le.classes_).plot(cmap='Blues')\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()\n",
    "\n",
    "# # ============================================================\n",
    "# # 1ï¸âƒ£1ï¸âƒ£ Learning Curves\n",
    "# # ============================================================\n",
    "# plt.figure(figsize=(8,5))\n",
    "# plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.title('Training vs Validation Accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d74f671-f35b-4293-b5c1-df8b8296d221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"yasserh/uber-fares-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d085be0b-98e3-407e-a107-915b564fcfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# ðŸš– UBER FARE PREDICTION â€” IMPROVED MODEL (XGBoost)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1ï¸âƒ£ LOAD DATA\n",
    "# ------------------------------------------------------------\n",
    "path = \"/home/spark-8bcba2ed-d14f-4c0a-9518-66/.cache/kagglehub/datasets/yasserh/uber-fares-dataset/versions/1/uber.csv\"\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2ï¸âƒ£ CLEAN DATA\n",
    "# ------------------------------------------------------------\n",
    "# Drop missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Keep realistic fare amounts\n",
    "df = df[(df['fare_amount'] > 0) & (df['fare_amount'] < 200)]\n",
    "\n",
    "# Keep valid coordinates (approx NYC range)\n",
    "df = df[\n",
    "    (df['pickup_longitude'] > -80) & (df['pickup_longitude'] < -70) &\n",
    "    (df['dropoff_longitude'] > -80) & (df['dropoff_longitude'] < -70) &\n",
    "    (df['pickup_latitude'] > 35) & (df['pickup_latitude'] < 45) &\n",
    "    (df['dropoff_latitude'] > 35) & (df['dropoff_latitude'] < 45)\n",
    "]\n",
    "\n",
    "print(\"After cleaning:\", df.shape)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3ï¸âƒ£ FEATURE ENGINEERING\n",
    "# ------------------------------------------------------------\n",
    "# Convert pickup_datetime to datetime\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], errors='coerce')\n",
    "\n",
    "# Extract useful datetime parts\n",
    "df['hour'] = df['pickup_datetime'].dt.hour\n",
    "df['day'] = df['pickup_datetime'].dt.day\n",
    "df['month'] = df['pickup_datetime'].dt.month\n",
    "df['weekday'] = df['pickup_datetime'].dt.weekday\n",
    "\n",
    "# Compute approximate distance (Haversine or Euclidean simplified)\n",
    "df['distance_km'] = np.sqrt(\n",
    "    (df['dropoff_longitude'] - df['pickup_longitude'])**2 +\n",
    "    (df['dropoff_latitude'] - df['pickup_latitude'])**2\n",
    ") * 111  # rough conversion degrees â†’ km\n",
    "\n",
    "# Add interaction feature\n",
    "df['distance_per_passenger'] = df['distance_km'] / df['passenger_count']\n",
    "\n",
    "# Drop rows with infinite or NaN after feature creation\n",
    "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4ï¸âƒ£ DEFINE FEATURES AND TARGET\n",
    "# ------------------------------------------------------------\n",
    "features = [\n",
    "    'pickup_longitude', 'pickup_latitude',\n",
    "    'dropoff_longitude', 'dropoff_latitude',\n",
    "    'passenger_count', 'distance_km', 'hour', 'day', 'month', 'weekday', 'distance_per_passenger'\n",
    "]\n",
    "X = df[features]\n",
    "y = df['fare_amount']\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5ï¸âƒ£ TRAIN-TEST SPLIT\n",
    "# ------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6ï¸âƒ£ TRAIN XGBOOST MODEL\n",
    "# ------------------------------------------------------------\n",
    "model = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7ï¸âƒ£ PREDICT AND EVALUATE\n",
    "# ------------------------------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "accuracy = r2 * 100\n",
    "\n",
    "print(f\"\\nðŸ“Š Model Performance:\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"RÂ²:   {r2:.3f}  ({accuracy:.2f}% accuracy)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8ï¸âƒ£ VISUALIZATIONS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Actual vs Predicted Scatter\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([0,200], [0,200], color='black', linewidth=2)\n",
    "plt.xlabel(\"Actual Fare ($)\")\n",
    "plt.ylabel(\"Predicted Fare ($)\")\n",
    "plt.title(\"Uber Fare Prediction: Actual vs Predicted\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy Bar Chart\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.bar(['Model Accuracy'], [accuracy], color='mediumseagreen')\n",
    "plt.ylim(0, 100)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title(f'Uber Fare Prediction Accuracy: {accuracy:.2f}%')\n",
    "plt.show()\n",
    "\n",
    "# Residuals Plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Fare ($)')\n",
    "plt.ylabel('Residual (Actual - Predicted)')\n",
    "plt.title('Residuals Plot')\n",
    "plt.show()\n",
    "\n",
    "# Distribution Comparison\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.hist(y_test, bins=50, alpha=0.6, label='Actual Fares')\n",
    "plt.hist(y_pred, bins=50, alpha=0.6, label='Predicted Fares')\n",
    "plt.xlabel('Fare Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution: Actual vs Predicted Uber Fares')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Done! The model is trained, evaluated, and visualized successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-11-02 09_50_29",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
