{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16c1466-c75d-4f7f-a637-5dd936f5a041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# csv_path = \"/Volumes/workspace/mais_hacks/dataset/*.csv\"\n",
    "# df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# # Columns to keep (use the exact names in your CSV)\n",
    "# cols_to_keep = [\"Date/Time (LST)\", \"Temp (�C)\"]\n",
    "# df = df.select([F.col(c) for c in cols_to_keep])\n",
    "\n",
    "# # Convert Date/Time to proper timestamp\n",
    "# df = df.withColumn(\n",
    "#     \"timestamp\",\n",
    "#     F.to_timestamp(F.col(\"Date/Time (LST)\"), \"yyyy-MM-dd HH:mm\")  # adjust format if needed\n",
    "# )\n",
    "\n",
    "# # Rename temperature column to a clean name\n",
    "# df = df.withColumnRenamed(\"Temp (�C)\", \"temp\")\n",
    "\n",
    "# # Keep only timestamps and temp\n",
    "# df_weather = df.select(\"timestamp\", \"temp\")\n",
    "\n",
    "# # Save as table\n",
    "# df_weather.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"mais_hacks.mais_hacks_cleaned2\")\n",
    "\n",
    "# df_weather.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dabf3e47-cae1-4088-bb81-dbb0c184e3a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # Read arrivals CSV\n",
    "# csv_path2 = \"/Volumes/workspace/mais_hacks/scnd/*.csv\"\n",
    "# df_arrival = spark.read.csv(csv_path2, header=True, inferSchema=True)\n",
    "\n",
    "# # Inspect columns to find timestamp column\n",
    "# df_arrival.printSchema()\n",
    "# df_arrival.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3530219e-4d80-479e-9de0-69a2a87fe650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Truncate timestamps to the hour for alignment\n",
    "# df_arrival_hourly = df_arrival.withColumn(\"hour_ts\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "\n",
    "# df_weather_hourly = df_weather.withColumn(\"hour_ts\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "\n",
    "# # Join on the hourly timestamp\n",
    "# combined_df = df_arrival_hourly.join(\n",
    "#     df_weather_hourly,\n",
    "#     on=\"hour_ts\",\n",
    "#     how=\"left\"\n",
    "# ).drop(\"timestamp\") \\\n",
    "#  .withColumnRenamed(\"hour_ts\", \"timestamp\")\n",
    "\n",
    "# combined_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c18c0949-5d42-417b-9171-a387b0df3cb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Check column names to be sure\n",
    "# combined_df.printSchema()\n",
    "\n",
    "# # Use the correct timestamp column name ('timestamp')\n",
    "# combined_df = combined_df.withColumn(\"year\", F.year(F.col(\"timestamp\")))\n",
    "\n",
    "# # Training data: 2022-2023\n",
    "# train_df = combined_df.filter((F.col(\"year\") >= 2022) & (F.col(\"year\") <= 2023)).drop(\"year\")\n",
    "\n",
    "# # Testing data: 2024\n",
    "# test_df = combined_df.filter(F.col(\"year\") == 2024).drop(\"year\")\n",
    "\n",
    "# print(\"Training data count:\", train_df.count())\n",
    "# print(\"Testing data count:\", test_df.count())\n",
    "\n",
    "# train_df.show(5)\n",
    "# test_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db47921-187f-4e55-83ba-003160ae4f06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# ----------------------------\n",
    "# Step 1: Read CSVs\n",
    "# ----------------------------\n",
    "csv_weather = \"/Volumes/workspace/mais_hacks/dataset/*.csv\"\n",
    "csv_arrivals = \"/Volumes/workspace/mais_hacks/scnd/*.csv\"\n",
    "\n",
    "df_weather = spark.read.csv(csv_weather, header=True, inferSchema=True)\n",
    "df_arrivals = spark.read.csv(csv_arrivals, header=True, inferSchema=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 2: Clean weather data\n",
    "# ----------------------------\n",
    "df_weather = df_weather.select(\n",
    "    F.col(\"Date/Time (LST)\").alias(\"timestamp\"),\n",
    "    F.col(\"Temp (�C)\").alias(\"temp\")\n",
    ")\n",
    "\n",
    "df_weather = df_weather.withColumn(\n",
    "    \"timestamp\",\n",
    "    F.to_timestamp(F.col(\"timestamp\"), \"yyyy-MM-dd HH:mm\")\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 3: Clean arrivals data\n",
    "# ----------------------------\n",
    "# Rename your timestamp column if needed\n",
    "# Replace 'timestamps' below with actual column name in arrivals CSV\n",
    "df_arrivals = df_arrivals.withColumnRenamed(\"timestamps\", \"timestamp\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 4: Align to hourly timestamps\n",
    "# ----------------------------\n",
    "df_weather_hourly = df_weather.withColumn(\"hour_ts\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "df_arrivals_hourly = df_arrivals.withColumn(\"hour_ts\", F.date_trunc(\"hour\", F.col(\"timestamp\")))\n",
    "\n",
    "# ----------------------------\n",
    "# Step 5: Join weather and arrivals\n",
    "# ----------------------------\n",
    "combined_df = df_arrivals_hourly.join(\n",
    "    df_weather_hourly,\n",
    "    on=\"hour_ts\",\n",
    "    how=\"left\"\n",
    ").drop(\"timestamp\") \\\n",
    " .withColumnRenamed(\"hour_ts\", \"timestamp\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 6: Add day-of-week\n",
    "# ----------------------------\n",
    "combined_df = combined_df.withColumn(\"day_of_week_int\", F.dayofweek(F.col(\"timestamp\")))\n",
    "combined_df = combined_df.withColumn(\"day_of_week\", F.date_format(F.col(\"timestamp\"), \"EEEE\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Step 7: Add holidays\n",
    "# ----------------------------\n",
    "holidays = [\n",
    "    \"2022-01-01\", \"2022-07-01\", \"2022-12-25\",\n",
    "    \"2023-01-01\", \"2023-07-01\", \"2023-12-25\",\n",
    "    \"2024-01-01\", \"2024-07-01\", \"2024-12-25\"\n",
    "]\n",
    "\n",
    "# Create a holidays DataFrame\n",
    "holidays_df = spark.createDataFrame([(d,) for d in holidays], [\"holiday_date\"])\n",
    "holidays_df = holidays_df.withColumn(\"holiday_date\", F.to_date(\"holiday_date\", \"yyyy-MM-dd\"))\n",
    "\n",
    "# Extract date from timestamp\n",
    "combined_df = combined_df.withColumn(\"date_only\", F.to_date(\"timestamp\"))\n",
    "\n",
    "# Join to flag holidays\n",
    "combined_df = combined_df.join(\n",
    "    holidays_df.withColumn(\"is_holiday\", F.lit(1)),\n",
    "    combined_df[\"date_only\"] == holidays_df[\"holiday_date\"],\n",
    "    how=\"left\"\n",
    ").drop(\"holiday_date\")\n",
    "\n",
    "combined_df = combined_df.fillna({\"is_holiday\": 0}).drop(\"date_only\")\n",
    "\n",
    "# ----------------------------\n",
    "# Step 8: Split train/test by year\n",
    "# ----------------------------\n",
    "combined_df = combined_df.withColumn(\"year\", F.year(F.col(\"timestamp\")))\n",
    "\n",
    "train_df = combined_df.filter((F.col(\"year\") >= 2022) & (F.col(\"year\") <= 2023)).drop(\"year\")\n",
    "test_df = combined_df.filter(F.col(\"year\") == 2024).drop(\"year\")\n",
    "\n",
    "print(\"Training data count:\", train_df.count())\n",
    "print(\"Testing data count:\", test_df.count())\n",
    "\n",
    "train_df.show(5)\n",
    "test_df.show(5)\n",
    "\n",
    "# ----------------------------\n",
    "# Step 9: Optional save as table\n",
    "# ----------------------------\n",
    "# combined_df.write.mode(\"overwrite\").saveAsTable(\"mais_hacks.mais_hacks_combined\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
